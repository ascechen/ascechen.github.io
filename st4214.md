# ST5214: Advanced probablity

## Main stream
- **measurable space** why do we need axiomized probability? To solve some paradoxes and get rigorous treatment to probability, we need to be careful when defining proabability. For finite set it is always fine, but for infinite set, the strucure of power set is rather compliacted to define a proability over it. To strike a balance between interesing events and tractable events, we define the $\sigma$-algebra. In the definition, countable union needs additional thoughts. We can say it is something between finite and uncoutable and it is powerful enough to solve many probability problems. 

- **probability** After defining measurable space, we assign probability to each events in an appropriate way: nonnegativity, normalization, and countably additivity. The third requirement builds the connection to analysis, that is the root of power of modern probability. Certainly we find it is too tedious to define probability measure for all possible events. But fortunately and intuitively, Caratheodory's extension theorem saves us by claiming we can define proability over a ring where every opration is finite (no limit) and then extend it on the whole $\sigma$-algebra. Uniqueness can be questioned. 

- **random variable** Even we have defined proability space, it is awkard to compute the proability of something interesting happns by always writing down the events. Hence the definition of random variable is very natual and get us rid of messy events. To define a random variable, we need the preimage of interesting range over real numbers to be events (or measurable). Borel $\sigma$-algebra is a natural one on real numbers and it has been proved to be enough for most of our operations. So a r.v. is simply a measurable function from sample space to real numbers. Then the concept of distribution function (cdf) arises naturally and the three properties follows naturlly from properties of probability measure, especially the right continuity corresponds to countable additivity, bridging probability and analysis so clearly! Conversely, distribution function also determines the probability measure uniquely! Since it is easier to deal with real numbers and distribution function, we will focus on that in the following instead of always looking at the underlying probability space. 

- **independence** One interpretation of $\sigma$-algebra is information field. Basically, it describes how much information we get observe from it. Of course, the trivial $\sigma$-algebra gives no or trivial information and richer $\sigma$-algebra can provide more information. We can define sub-$\sigma$-algebra to describe certain level of information offered and investigate the relation of different sub-$\sigma$-algebra. Do they offer completely different information? How about the $\sigma$-algebra generated from the union of these two sub-$\sigma$-algebra? These questions motivate the concept of independence. Informally speaking, getting information by observing one sub-$\simga$-algebra does not help us know more by simply observing another, that is independence. To be rigorous, we need probability: the probability of intersection of two events from two sub-$\sigma$-algebra is the product of probabilities of two events. In addition, independence can be generalized to countably many sub-$\sigma$-algerba. Again, independence build the connection of proabilitya and analysis. It somehow simplify the calculation of probability of certaint events using decomposition. 


