# ST5214: Advanced probablity

## Main stream
- **measurable space** why do we need axiomized probability? To solve some paradoxes and get rigorous treatment to probability, we need to be careful when defining proabability. For finite set it is always fine, but for infinite set, the structure of power set is rather compliacted to define a proability over it. To strike a balance between interesing events and tractable events, we define the $\sigma$-algebra. In the definition, countable union needs additional thoughts. We can say it is something between finite and uncoutable and it is powerful enough to solve many probability problems. 

- **probability** After defining measurable space, we assign probability to each events in an appropriate way: nonnegativity, normalization, and countably additivity. The third requirement builds the connection to analysis, that is the root of power of modern probability. Certainly we find it is too tedious to define probability measure for all possible events. But fortunately and intuitively, Caratheodory's extension theorem saves us by claiming we can define proability over a ring where every opration is finite (no limit) and then extend it on the whole $\sigma$-algebra. Uniqueness can be questioned. 

- **measurable** We need to develop some intuition on "measurable". Basically, a set is measurable means we can observe it and distinguish it with other observeable set, then it is called an event. So $sigma$-algebra can be regarded as information field. The more sets it contains, the more we can observe. If we only care a little, we may only need a simple $\sigma$-algebra. For example, if we only care first coin toss in a sequence of coin tosses. And a map from one measurable space to another is called measurable means the pre-image of any measurble set is measurable. Usually the target measurable space is real numbers and the Borel $\sigma$-algebra over it. So a measurable function means it allows us to observe events mapped to exactly a Borel set, which means we can know the function well (or the function is nice from mesurable point of view). 

- **random variable** Even we have defined proability space, it is awkard to compute the proability of something interesting happns by always writing down the events. Hence the definition of random variable is very natual and get us rid of messy events. To define a random variable, we need the preimage of interesting range over real numbers to be events (or measurable). Borel $\sigma$-algebra is a natural one on real numbers and it has been proved to be enough for most of our operations. So a r.v. is simply a measurable function from sample space to real numbers with Borel $\sigam$-algebra. Then the concept of distribution function (cdf) arises naturally and the three properties follows naturlly from properties of probability measure, especially the right continuity corresponds to countable additivity, bridging probability and analysis so clearly! Conversely, distribution function also determines a pull-back probability measure on the reals with Borel $\sigam$-algebra uniquely! Since it is easier to deal with real numbers and distribution function, we will focus on either cdf or the real line with Borel $\sigam$-algebra and pull-back measure in the following instead of always looking at the underlying probability space.  

- **independence** One interpretation of $\sigma$-algebra is information field. Basically, it describes how much information we get observe from it. Of course, the trivial $\sigma$-algebra gives no or trivial information and richer $\sigma$-algebra can provide more information. We can define sub-$\sigma$-algebra to describe certain level of information offered and investigate the relation of different sub-$\sigma$-algebra. Do they offer completely different information? How about the $\sigma$-algebra generated from the union of these two sub-$\sigma$-algebra? These questions motivate the concept of independence. Informally speaking, getting information by observing one sub-$\simga$-algebra does not help us know more by simply observing another, that is independence. To be rigorous, we need probability: the probability of intersection of two events from two sub-$\sigma$-algebra is the product of probabilities of two events. In addition, independence can be generalized to countably many sub-$\sigma$-algerba. Again, independence build the connection of proabilitya and analysis. It somehow simplify the calculation of probability of certaint events using decomposition. After defining independence of sigma-algebra, we can define independence of r.v. based on sigma-algebra generated by r.v. which is the necessary information to know the r.v.. Finally, note that pair-wise independence does not imply mutually independence, an example is two coin tosses. 

**Construct random variables** Given the definition of r.v., we can ask whether a function is r.v. and what the distribution function is. To answer the first, we only need to check the pre-image of any half line since it generates Borel-$\sigma$-algebra.  Note that only checking for pre-image of singleton is not enough since it does not generate Borel-$\sigma$-algebra. For the second, try to express set as disjoint union. Those are general approach by definition. Similar to continuity or convexity, we can find basic operations preserving measurability. From infinite coin tosses we can construct a uniform distribution based on binary expansion of real numbers between 0 and 1. Then from this equivalence we can extract a sequence of independent uniform r.v. Then we can define any r.v. we need. Suprisingly, uniform distribution is much more complicated than we imagine. So we can see the modern probability theory is built onto infinite indepent uniform r.v., or infinite sequence of coin tosses. 






