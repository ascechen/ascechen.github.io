# Review for ST5241: Concentration Inequalities and Emprical Process Methods for High-Dimensional Statistics

This module is mainly concerned with the behavior of objects with much randomness. More speficivally, we are interested in objects such as 
functions or even mappings depending on many random variables. For example, the average, or the maximum, or a convex function of a bunch of
random variables. 

## What we study
functions/mappings like 
  - summation, average or linear function
  - quadratic form or functions
  - maximum or supreme or piecewise linear convex functions
  - norm 
  - convex functions 
  - Lipchitz functions 
  - spectum 
  - ... 
  
of objects like 
  - random vectors
  - random matrices 
  - random processes 
  - ...
  
Since the underlying randomness lies in a high-dim space, the components could be independent or dependent. For example, when study the 
classical CLT, we usually require independence. But in a stochastic process like Markov chain, the r.v.s usually have strong dependence. 
To understand random objects, we want to know the probability distributions. However, that is often too much to expect. Instead, we usually 
want to know certain information, like support, mean, variance, tail probabilities, etc to understand certain behavior, especially magnitude
and concentration of these random objects. Hence, we need to develop tools to understand these statistical information of random objects.

## Motivation
- Essentially this module involves everything about randomness, but the main taste should be the concenration phenomenon and empirical
processes. These topics are very related to how to analyze randomized algorithm, especially those arise from machine learning. 
- Persoanlly, I am interested in optimization under uncertainty. In stochastic programming we need to make feasible decisions and such decisions and 
some random varaibles will affect our objective, sometimes the decisions can even affect the random variables (endogeouos uncertainty). 
This module will help us understand the randomness and develop approximaitons/algorithms to do optimization. 
- These topics are interesting of their own interests. There are many interesting ideas and they may come from different research communities
including math, statistics,  EECS, IEOR, etc. 

## What shall we know? The contents. 
### Mean estimation
For a linear funciton, this is easy. When it comes to nonlinear world, it is nontrival. We are mainly concernted with supreme

### Concentrations
For functions of independent r.v., what we can expect is as long as the function is not very sensitive to each component and each component
is well concentrated, then the function should be quite concentrated. Sometimes even though the components are not well-behaved, the function
can also be well concentrated since some randomness cancel out when there are much randomness. 




## How to study: ideas/intutions and  methods/techniques
### Concentration inequalities:
- Chernoff's methods: require inependence; build connection from moments to tail probabilities. 
  - 

  
 
  
